{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMytWUkASxr/XIItMb+EkLn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhaoxiaoer/ml/blob/main/src/2020/hw4/hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fslwyTsGJthr"
      },
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "本次作业是要让同学接触 NLP 当中一个简单的 task -- 语句分类（文本分类）\n",
        "\n",
        "给定一个语句，判断他有没有恶意（负面标 1，正面标 0）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpZc8MHCJj9y",
        "outputId": "744eb70f-ba08-4d4c-d30f-6662c9c3fe9f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7ArKg8eKpgB",
        "outputId": "505c0521-032d-4d5b-907c-d05955d81455"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/2020/hw4\n",
        "!ls\n",
        "path_prefix = './'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/2020/hw4\n",
            "data.zip   testing_data.txt    training_nolabel.txt\n",
            "hw4.ipynb  training_label.txt  w2v_all.model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA3uY3yULzUr"
      },
      "source": [
        "### Download Dataset\n",
        "\n",
        "有三个档案，分别是 training_label.txt、training_nolabel.txt、testing_data.txt\n",
        "\n",
        "- training_label.txt：有 label 的 training data (句子配上 0 or 1, +++$+++ 只是分隔符号，不要理它)\n",
        "      - e.g., 1 +++$+++ are wtf ... awww thanks !\n",
        "\n",
        "- training_nolabel.txt：没有 label 的 training data (只有句子)，用来做 semi-supervised learning\n",
        "      - ex: hates being this burnt !! ouch\n",
        "\n",
        "- testing_data.txt：你要判断 testing data 里面的句子是 0 or 1\n",
        "\n",
        "    >id,text\n",
        "\n",
        "    >0,my dog ate our dinner . no, seriously ... he ate it .\n",
        "\n",
        "    >1,omg last day sooon n of primary noooooo x im gona be swimming out of school wif the amount of tears am gona cry\n",
        "    \n",
        "    >2,stupid boys .. they ' re so .. stupid !"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Db3suTvmt4K",
        "outputId": "90f00aae-cddc-4eba-8e82-cd2556c55a25"
      },
      "source": [
        "!gdown --id '1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8' --output data.zip\n",
        "!unzip data.zip\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8\n",
            "To: /content/drive/My Drive/Colab Notebooks/2020/hw4/data.zip\n",
            "45.1MB [00:00, 107MB/s] \n",
            "Archive:  data.zip\n",
            "  inflating: training_label.txt      \n",
            "  inflating: testing_data.txt        \n",
            "  inflating: training_nolabel.txt    \n",
            "data.zip  hw4.ipynb  testing_data.txt  training_label.txt  training_nolabel.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOwj85D4TQQg",
        "outputId": "7357a1d8-fc33-4f06-95c3-3f309c7e38e2"
      },
      "source": [
        "with open('testing_data.txt', 'r') as f:\n",
        "    line = f.readline()\n",
        "    line = f.readline()\n",
        "    print(line)\n",
        "    # print(line.strip('\\n').split(' '))\n",
        "    # print(line.strip('\\n').split(' ')[2:])\n",
        "    print(line.strip('\\n').split(','))\n",
        "    print(line.strip('\\n').split(',')[1:])\n",
        "    print(\"\".join(line.strip('\\n').split(',')[1:]))\n",
        "    print(\"\".join(line.strip('\\n').split(',')[1:]).strip())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0,my dog ate our dinner . no , seriously ... he ate it .\n",
            "\n",
            "['0', 'my dog ate our dinner . no ', ' seriously ... he ate it .']\n",
            "['my dog ate our dinner . no ', ' seriously ... he ate it .']\n",
            "my dog ate our dinner . no  seriously ... he ate it .\n",
            "my dog ate our dinner . no  seriously ... he ate it .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFCUxJMfm8be"
      },
      "source": [
        "# this is for filtering the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XYylkTL_LeI"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEqQjznv_Nrp"
      },
      "source": [
        "# utils.py\n",
        "# 这个 block 用来定义一些常用的函数\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def load_training_data(path='training_label.txt'):\n",
        "    # 把 training 时需要的 data 读进来\n",
        "    # 如果是 'training_label.txt'，需要读取 label，如果是 'training_nolabel.txt'，不需要读取 label\n",
        "    if 'training_label' in path:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            lines = [line.strip('\\n').split(' ') for line in lines]\n",
        "        x = [line[2:] for line in lines]\n",
        "        y = [line[0] for line in lines]\n",
        "        return x, y\n",
        "    else:\n",
        "        with open(path, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            x = [line.strip('\\n').split(' ') for line in lines]\n",
        "        return x\n",
        "\n",
        "def load_testing_data(path='testing_data.txt'):\n",
        "    # 把 testing 时需要的 data 读进来\n",
        "    with open(path, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        X = [\"\".join(line.strip('\\n').split(',')[1:]).strip() for line in lines[1:]]\n",
        "        X = [sen.split(' ') for sen in X]\n",
        "    return X\n",
        "\n",
        "def evaluation(outputs, labels):\n",
        "    # outputs => probability (float)\n",
        "    # labels => labels\n",
        "    outputs[outputs>=0.5] = 1  # 大于等于 0.5 为正面\n",
        "    outputs[outputs<0.5] = 0  # 小于 0.5 为负面\n",
        "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
        "    return correct"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJNi5OoJWU0t",
        "outputId": "65073d0e-f67e-4dad-9afb-4ab61bc47aa3"
      },
      "source": [
        "outputs = torch.tensor([[0.8], [0.3], [0.6]])\n",
        "print(outputs)\n",
        "print(outputs>=0.5)\n",
        "print(outputs[outputs>=0.5])\n",
        "outputs[outputs>=0.5] = 1\n",
        "print(outputs)\n",
        "print(outputs.size())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8000],\n",
            "        [0.3000],\n",
            "        [0.6000]])\n",
            "tensor([[ True],\n",
            "        [False],\n",
            "        [ True]])\n",
            "tensor([0.8000, 0.6000])\n",
            "tensor([[1.0000],\n",
            "        [0.3000],\n",
            "        [1.0000]])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYVF4UQecsG3"
      },
      "source": [
        "### Train Word to Vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZqcu1DtcwhL",
        "outputId": "2ea31e05-f8e5-47d6-f959-30d968829a04"
      },
      "source": [
        "# w2v.py\n",
        "# 这个 block 是用来训练 word to vector 的 word embedding\n",
        "# 注意！这个 block 在训练 word to vector 时是用 cpu，可能要花费 10 分钟以上\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import argparse\n",
        "from gensim.models import word2vec\n",
        "\n",
        "def train_word2vec(x):\n",
        "    # 训练 word to vector 的 word embedding\n",
        "    # sentences: 可以是一个 list，对于大语料集，建议使用 BrownCorpus, Text8Corpus 或 lineSentence 构建\n",
        "    # size: 特征向量的维度，默认100\n",
        "    # window: 窗口大小，表示当前词与预测词在一个句子中的最大距离是多少\n",
        "    # min_count: 可以对字典做截断，词频少于 min_count 次数的单词会被丢掉，默认值5\n",
        "    # max_vocab_size: 设置词向量构建期间的 RAM 限制，设置成 None 则没有限制\n",
        "    # workers: 并发训练时的线程数\n",
        "    # sg: 用于设置训练算法，默认是0，对应 CBOW 算法，1 对应 skip-gram 算法\n",
        "    # iter: 迭代次数，默认是5\n",
        "    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)\n",
        "    return model\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"loading training data ...\")\n",
        "    train_x, y = load_training_data('training_label.txt')\n",
        "    print(len(train_x))\n",
        "    print(type(train_x))\n",
        "    print(train_x[0])\n",
        "    print(y[0])\n",
        "    train_x_no_label = load_training_data('training_nolabel.txt')\n",
        "\n",
        "    print(\"loading testing data ...\")\n",
        "    test_x = load_testing_data('testing_data.txt')\n",
        "\n",
        "    # model = train_word2vec(train_x + train_x_no_label + test_x)\n",
        "    model = train_word2vec(train_x + test_x)\n",
        "\n",
        "    print(\"saving model ...\")\n",
        "    model.save(os.path.join(path_prefix, 'w2v_all.model'))  # 模型保存"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading training data ...\n",
            "200000\n",
            "<class 'list'>\n",
            "['are', 'wtf', '...', 'awww', 'thanks', '!']\n",
            "1\n",
            "loading testing data ...\n",
            "saving model ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5PXzcfEw8sV",
        "outputId": "9afab96d-8f8b-4c22-9343-1f5f1a8b5b2f"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "model2 = Word2Vec.load('w2v_all.model')  # 模型读取\n",
        "print(model2.vector_size)\n",
        "print(type(model2['dog']))\n",
        "print(len(model2['dog']))\n",
        "# print(model2['dog'])\n",
        "print(model2.similarity('dog', 'cat'))  # 相似性\n",
        "print(model2.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))  # 最相似\n",
        "print(model2.most_similar(['man']))\n",
        "print(model2.doesnt_match(\"breakfast cereal dinner lunch\".split()))  # 最不相似\n",
        "print(type(model2.wv.vocab))\n",
        "print(len(model2.wv.vocab))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250\n",
            "<class 'numpy.ndarray'>\n",
            "250\n",
            "0.57638174\n",
            "[('devil', 0.3734397292137146)]\n",
            "[('dude', 0.5109483003616333), ('lawd', 0.4925040304660797), ('mang', 0.4771396219730377), ('maaan', 0.47376304864883423), ('guy', 0.45980724692344666), ('wellz', 0.44957679510116577), ('flyy', 0.4483198821544647), ('raul', 0.44600674510002136), ('jeez', 0.44435152411460876), ('babe', 0.4417971670627594)]\n",
            "cereal\n",
            "<class 'dict'>\n",
            "24694\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  after removing the cwd from sys.path.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  import sys\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiQkGzwBkavA"
      },
      "source": [
        "### Data Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba1D7IaDkiib"
      },
      "source": [
        "# preprocess.py\n",
        "# 这个 block 用来做 data 的预处理\n",
        "from torch import nn\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "class Preprocess():\n",
        "    def __init__(self, sentences, sen_len, w2v_path=\"./w2v_all.model\"):\n",
        "        self.w2v_path = w2v_path\n",
        "        self.sentences = sentences\n",
        "        self.sen_len = sen_len\n",
        "        self.idx2word = []\n",
        "        self.word2idx = {}\n",
        "        self.embedding_matrix = []\n",
        "    def get_w2v_model(self):\n",
        "        # 把之前训练好的 word to vec 模型读进来\n",
        "        self.embedding = Word2Vec.load(self.w2v_path)\n",
        "        self.embedding_dim = self.embedding.vector_size\n",
        "    def add_embedding(self, word):  # 添加两个额外的单词，用于\"填充\"和\"未定义\"\n",
        "        # 把 word 加进 embedding，并赋予它一个随机生成的 representation vector\n",
        "        # word 只会是 \"<PAD>\" 或 \"<UNK>\"\n",
        "        vector = torch.empty(1, self.embedding_dim)\n",
        "        torch.nn.init.uniform_(vector)\n",
        "        self.word2idx[word] = len(self.word2idx)\n",
        "        self.idx2word.append(word)\n",
        "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
        "    def make_embedding(self, load=True):\n",
        "        print(\"Get embedding ...\")\n",
        "        # 获取训练好的 Word2Vec word embedding\n",
        "        if load:\n",
        "            print(\"loading word to vec model ...\")\n",
        "            self.get_w2v_model()\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        # 制作一个 word2idx 的 dictionary\n",
        "        # 制作一个 idx2word 的 list\n",
        "        # 制作一个 word2vector 的 list\n",
        "        for i, word in enumerate(self.embedding.wv.vocab):  # 模型的词向量\n",
        "            print('get words #{}'.format(i+1), end='\\r')\n",
        "            # e.g. self.word2idx['he'] = 1\n",
        "            # e.g. self.idx2word[1] = 'he'\n",
        "            # e.g. slef.vectors[1] = 'he' vector\n",
        "            self.word2idx[word] = len(self.word2idx)\n",
        "            self.idx2word.append(word)\n",
        "            self.embedding_matrix.append(self.embedding[word])\n",
        "        print('')\n",
        "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
        "        # 将 \"<PAD>\" 和 \"<UNK>\" 加入 embedding 里面\n",
        "        self.add_embedding(\"<PAD>\")\n",
        "        self.add_embedding(\"<UNK>\")\n",
        "        print(\"total words: {}\".format(len(self.embedding_matrix)))\n",
        "        return self.embedding_matrix\n",
        "    def pad_sequence(self, sentence):\n",
        "        # 将每个句子变成一样的长度\n",
        "        if len(sentence) > self.sen_len:\n",
        "            sentence = sentence[:self.sen_len]\n",
        "        else:\n",
        "            pad_len = self.sen_len - len(sentence)\n",
        "            for _ in range(pad_len):\n",
        "                sentence.append(self.word2idx[\"<PAD>\"])\n",
        "        assert len(sentence) == self.sen_len\n",
        "        return sentence\n",
        "    def sentence_word2idx(self):\n",
        "        sentence_list = []\n",
        "        for i, sen in enumerate(self.sentences):\n",
        "            print('sentence count #{}'.format(i+1), end='\\r')\n",
        "            sentence_idx = []\n",
        "            for word in sen:\n",
        "                if (word in self.word2idx.keys()):\n",
        "                    sentence_idx.append(self.word2idx[word])\n",
        "                else:\n",
        "                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n",
        "            # 将每个句子变成一样的长度\n",
        "            sentence_idx = self.pad_sequence(sentence_idx)\n",
        "            sentence_list.append(sentence_idx)\n",
        "        return torch.LongTensor(sentence_list)\n",
        "    def labels_to_tensor(self, y):\n",
        "        # 把 labels 转成 tensor\n",
        "        y = [int(label) for label in y]\n",
        "        return torch.LongTensor(y)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swd66B5cqZ8c",
        "outputId": "36c47ac9-0333-4ae5-cbf2-bfcea4e086c7"
      },
      "source": [
        "vector = torch.empty(1, 3)\n",
        "print(vector)\n",
        "torch.nn.init.uniform_(vector)\n",
        "print(vector)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[7.9301e+13, 3.0702e-41, 3.3631e-44]])\n",
            "tensor([[0.5401, 0.2763, 0.4586]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyURZdCwRQm2"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8mNW_aVRron"
      },
      "source": [
        "# data.py\n",
        "# 实现了 dataset 所需要的 '__init__', '__getitem__', '__len__'\n",
        "# 好让 dataloader 能使用\n",
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "class TwitterDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Expected data shape like:(data_num, data_len)\n",
        "    Data can be a list of numpy array or a list of lists\n",
        "    input data shape:(data_num, seq_len, feature_dim)\n",
        "\n",
        "    __len__ will return the number of data\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y):\n",
        "        self.data = X\n",
        "        self.label = y\n",
        "    def __getitem__(self, idx):\n",
        "        if self.label is None: return self.data[idx]\n",
        "        return self.data[idx], self.label[idx]\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkINUC4QUU-M"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIm5qZyuVQgs"
      },
      "source": [
        "# model.py\n",
        "# 这个 block 是训练的模型\n",
        "import torch\n",
        "from torch import nn\n",
        "class LSTM_Net(nn.Module):\n",
        "    def __init__(self, embedding, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
        "        super(LSTM_Net, self).__init__()\n",
        "        # 制作 embedding layer\n",
        "        self.embedding = torch.nn.Embedding(embedding.size(0), embedding.size(1))\n",
        "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
        "        # 是否将 embedding fix 住，如果 fix_embedding 为 False，在训练过程中，embedding 也会跟着被训练\n",
        "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
        "        self.embedding_dim = embedding.size(1)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.lstm = nn.LSTM(self.embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.classifier = nn.Sequential( nn.Dropout(dropout),\n",
        "                                         nn.Linear(hidden_dim, 1),\n",
        "                                         nn.Sigmoid() )\n",
        "    def forward(self, inputs):\n",
        "        inputs = self.embedding(inputs)\n",
        "        x, _ = self.lstm(inputs, None)\n",
        "        # x 的 dimension (batch, seq_len, hidden_dim)\n",
        "        # 取用 LSTM 最后一层的 hidden state （hiden state 等于 输出）\n",
        "        x = x[:, -1, :]\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ0si7kkyqIw"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z51u4FpUyr7Y"
      },
      "source": [
        "# train.py\n",
        "# 这个 block 是用来训练模型的\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n",
        "    model.train()  # 将 model 的模式设为 train，这样 optimizer 就可以更新 model 的参数\n",
        "    criterion = nn.BCELoss()  # 定义损失函数，这里我们使用 binary cross entropy loss\n",
        "    t_batch = len(train)\n",
        "    v_batch = len(valid)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)  # 将模型的参数给 optimizer，并给予适当的 learning rate\n",
        "    total_loss, total_acc, best_acc = 0, 0, 0\n",
        "    for epoch in range(n_epoch):\n",
        "        total_loss, total_acc = 0, 0\n",
        "        # 这段做 training\n",
        "        for i, (inputs, labels) in enumerate(train):\n",
        "            inputs = inputs.to(device, dtype=torch.long)  # device 为 \"cuda\"，将 inputs 转成 torch.cuda.LongTensor\n",
        "            labels = labels.to(device, dtype=torch.float)  # device 为 \"cuda\"，将 labels 转成 torch.cuda.FloatTensor，因为等会要喂入 criterion，所以类型须是 float\n",
        "            optimizer.zero_grad()  # 由于 loss.backward() 的 gradient 会累加，所以每次喂完一个 batch 后需要清零\n",
        "            outputs = model(inputs)  # 将 input 喂给模型\n",
        "            outputs = outputs.squeeze()  # 去掉维度为1的 dimension，好让 outputs 可以喂入 criterion()\n",
        "            loss = criterion(outputs, labels)  # 计算此时模型的 training loss\n",
        "            loss.backward()  # 算 loss 的 gradient\n",
        "            optimizer.step()  # 更新训练模型的参数\n",
        "            correct = evaluation(outputs, labels)\n",
        "            total_acc += (correct / batch_size)\n",
        "            total_loss += loss.item()\n",
        "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n",
        "                epoch+1, i+1, t_batch, loss.item(), correct*100/batch_size))\n",
        "        print('\\nTrain | Loss: {:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
        "\n",
        "        # 这段做 validation\n",
        "        model.eval()  # 将 model 的模式设为 eval，这样 model 的参数就会被固定住\n",
        "        with torch.no_grad():\n",
        "            total_loss, total_acc = 0, 0\n",
        "            for i, (inputs, labels) in enumerate(valid):\n",
        "                inputs = inputs.to(device, dtype=torch.long)\n",
        "                labels = labels.to(device, dtype=torch.float)\n",
        "                outputs = model(inputs)\n",
        "                outputs = outputs.squeeze()\n",
        "                loss = criterion(outputs, labels)\n",
        "                correct = evaluation(outputs, labels)\n",
        "                total_acc += (correct / batch_size)\n",
        "                total_loss += loss.item()\n",
        "            print(\"Valid | Loss: {:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
        "            if total_acc > best_acc:\n",
        "                # 如果 validation 的结果优于之前的所有结果，就把当下的模型保存下来，用于预测使用\n",
        "                best_acc = total_acc\n",
        "                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\n",
        "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
        "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
        "        print('------------------------------------')\n",
        "        model.train()  # 将 model 的模式设为 train，这样 optimizer 就可以更新 model 的参数 (因为刚刚设成 eval 模式)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XPZkUskHarw"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIGlqaU3HcDl"
      },
      "source": [
        "# test.py\n",
        "# 这个 block 用来对 testing_data.txt 做预测\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "def testing(test_loader, model, device):\n",
        "    model.eval()\n",
        "    ret_output = []\n",
        "    with torch.no_grad():\n",
        "        for i, inputs in enumerate(test_loader):\n",
        "            inputs = inputs.to(device, dtype=torch.long)\n",
        "            outputs = model(inputs)\n",
        "            outputs = outputs.squeeze()\n",
        "            outputs[outputs>=0.5] = 1  # 大于等于 0.5 为正面\n",
        "            outputs[outputs<0.5] = 0  # 小于 0.5 为负面\n",
        "            ret_output += outputs.int().tolist()\n",
        "    \n",
        "    return ret_output"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cnUIV_MJbAM"
      },
      "source": [
        "### Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8m956YMJcs2",
        "outputId": "fab466a3-f44a-4367-9ec8-52a46ecd70cd"
      },
      "source": [
        "# main.py\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# 通过 torch.cuda.is_available() 的返回值判断是否有 GPU 的环境，如果有的话 device 就设为 \"cuda\"，没有的话就设为 \"cpu\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 处理好各个 data 的路径\n",
        "train_with_label = os.path.join(path_prefix, 'training_label.txt')\n",
        "train_no_label = os.path.join(path_prefix, 'training_nolabel.txt')\n",
        "testing_data = os.path.join(path_prefix, 'testing_data.txt')\n",
        "\n",
        "w2v_path = os.path.join(path_prefix, 'w2v_all.model')  # 处理 word to vec model 的路径\n",
        "\n",
        "# 定义：句子长度、是否固定 embedding、batch 大小、训练几个 epoch、learning rate 的值、model 的文件夹路径\n",
        "sen_len = 20\n",
        "fix_embedding = True  # fix embedding during training\n",
        "batch_size = 128\n",
        "epoch = 50\n",
        "lr = 0.001\n",
        "model_dir = path_prefix\n",
        "\n",
        "print(\"loading data ...\")  # 把 'training_label.txt' 跟 'training_nolabel.txt' 读进来\n",
        "train_x, y = load_training_data(train_with_label)\n",
        "train_x_no_label = load_training_data(train_no_label)\n",
        "\n",
        "# 对 inputs 和 labels 做预处理\n",
        "preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "train_x = preprocess.sentence_word2idx()\n",
        "y = preprocess.labels_to_tensor(y)\n",
        "\n",
        "# 制作一个 model 对象\n",
        "model = LSTM_Net(embedding, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\n",
        "model = model.to(device)\n",
        "\n",
        "# 把 data 分为 training data 和 validation data (将一部分 training data 拿来当做 validation data)\n",
        "X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]\n",
        "\n",
        "# 把 data 做成 dataset 供 dataloader 使用\n",
        "train_dataset = TwitterDataset(X=X_train, y=y_train)\n",
        "val_dataset = TwitterDataset(X=X_val, y=y_val)\n",
        "\n",
        "# 把 data 转成 batch of tensors\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=8)\n",
        "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         shuffle=False,\n",
        "                                         num_workers=8)\n",
        "\n",
        "# 开始训练\n",
        "training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data ...\n",
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #24694\n",
            "total words: 24696\n",
            "\n",
            "start training, parameter total:6415351, trainable:241351\n",
            "\n",
            "\n",
            "Train | Loss: 0.50090 Acc: 74.716\n",
            "Valid | Loss: 0.45716 Acc: 77.772 \n",
            "saving model with acc 77.772\n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.44328 Acc: 79.117\n",
            "Valid | Loss: 0.44154 Acc: 79.051 \n",
            "saving model with acc 79.051\n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.42744 Acc: 80.098\n",
            "Valid | Loss: 0.42908 Acc: 79.722 \n",
            "saving model with acc 79.722\n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.41467 Acc: 80.806\n",
            "Valid | Loss: 0.42589 Acc: 80.011 \n",
            "saving model with acc 80.011\n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.40243 Acc: 81.388\n",
            "Valid | Loss: 0.42328 Acc: 80.140 \n",
            "saving model with acc 80.140\n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.39062 Acc: 82.099\n",
            "Valid | Loss: 0.43337 Acc: 79.593 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.37857 Acc: 82.701\n",
            "Valid | Loss: 0.42494 Acc: 80.523 \n",
            "saving model with acc 80.523\n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.36381 Acc: 83.597\n",
            "Valid | Loss: 0.42664 Acc: 80.225 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.34690 Acc: 84.402\n",
            "Valid | Loss: 0.43919 Acc: 80.021 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.32937 Acc: 85.312\n",
            "Valid | Loss: 0.44802 Acc: 79.707 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.30895 Acc: 86.376\n",
            "Valid | Loss: 0.48729 Acc: 79.469 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.28829 Acc: 87.378\n",
            "Valid | Loss: 0.48240 Acc: 78.921 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.26644 Acc: 88.501\n",
            "Valid | Loss: 0.51408 Acc: 78.886 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.24541 Acc: 89.523\n",
            "Valid | Loss: 0.54936 Acc: 78.861 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.22573 Acc: 90.430\n",
            "Valid | Loss: 0.58660 Acc: 78.588 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.20631 Acc: 91.340\n",
            "Valid | Loss: 0.70252 Acc: 77.921 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.19160 Acc: 92.063\n",
            "Valid | Loss: 0.67724 Acc: 78.105 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.17558 Acc: 92.817\n",
            "Valid | Loss: 0.75763 Acc: 77.637 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.16454 Acc: 93.299\n",
            "Valid | Loss: 0.79836 Acc: 77.826 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.15331 Acc: 93.806\n",
            "Valid | Loss: 0.81441 Acc: 77.488 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.14317 Acc: 94.282\n",
            "Valid | Loss: 0.83752 Acc: 77.553 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.13237 Acc: 94.770\n",
            "Valid | Loss: 0.92849 Acc: 76.732 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.12511 Acc: 95.051\n",
            "Valid | Loss: 0.90822 Acc: 76.443 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.11838 Acc: 95.428\n",
            "Valid | Loss: 0.89819 Acc: 76.836 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.11131 Acc: 95.676\n",
            "Valid | Loss: 0.98282 Acc: 76.846 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.10311 Acc: 96.031\n",
            "Valid | Loss: 1.03859 Acc: 76.747 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.10035 Acc: 96.128\n",
            "Valid | Loss: 0.96323 Acc: 76.622 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.09661 Acc: 96.311\n",
            "Valid | Loss: 1.03244 Acc: 76.617 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.08972 Acc: 96.591\n",
            "Valid | Loss: 1.06630 Acc: 76.901 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.08695 Acc: 96.671\n",
            "Valid | Loss: 1.07586 Acc: 76.742 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.08369 Acc: 96.807\n",
            "Valid | Loss: 1.19149 Acc: 76.189 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.07987 Acc: 96.988\n",
            "Valid | Loss: 1.08078 Acc: 76.314 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.07916 Acc: 97.012\n",
            "Valid | Loss: 1.10635 Acc: 76.607 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.07423 Acc: 97.278\n",
            "Valid | Loss: 1.19324 Acc: 76.866 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.07272 Acc: 97.294\n",
            "Valid | Loss: 1.14817 Acc: 76.602 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.06688 Acc: 97.505\n",
            "Valid | Loss: 1.21719 Acc: 76.020 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.06861 Acc: 97.465\n",
            "Valid | Loss: 1.26099 Acc: 76.727 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.06502 Acc: 97.606\n",
            "Valid | Loss: 1.28263 Acc: 76.632 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.06555 Acc: 97.590\n",
            "Valid | Loss: 1.23080 Acc: 76.687 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.06015 Acc: 97.797\n",
            "Valid | Loss: 1.20432 Acc: 76.707 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.05880 Acc: 97.864\n",
            "Valid | Loss: 1.27098 Acc: 76.562 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.05984 Acc: 97.789\n",
            "Valid | Loss: 1.29088 Acc: 76.468 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.05798 Acc: 97.864\n",
            "Valid | Loss: 1.26928 Acc: 76.647 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.05487 Acc: 97.981\n",
            "Valid | Loss: 1.44617 Acc: 76.538 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.05587 Acc: 97.965\n",
            "Valid | Loss: 1.31832 Acc: 76.642 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.05335 Acc: 98.047\n",
            "Valid | Loss: 1.36116 Acc: 76.752 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.05455 Acc: 98.000\n",
            "Valid | Loss: 1.29179 Acc: 76.339 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.05420 Acc: 98.048\n",
            "Valid | Loss: 1.34750 Acc: 76.473 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.05043 Acc: 98.152\n",
            "Valid | Loss: 1.39829 Acc: 76.259 \n",
            "------------------------------------\n",
            "\n",
            "Train | Loss: 0.04999 Acc: 98.158\n",
            "Valid | Loss: 1.36635 Acc: 76.468 \n",
            "------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IohUXSTzaLM"
      },
      "source": [
        "### Predict and Write to csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SGABMPRzeCC",
        "outputId": "f3ea8a6f-7ed2-47ec-a10e-a32727a1774e"
      },
      "source": [
        "# 开始测试模型并进行预测\n",
        "print(\"loading testing data ...\")\n",
        "test_x = load_testing_data(testing_data)\n",
        "preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n",
        "embedding = preprocess.make_embedding(load=True)\n",
        "test_x = preprocess.sentence_word2idx()\n",
        "test_dataset = TwitterDataset(X=test_x, y=None)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=8)\n",
        "print('\\nload model ...')\n",
        "model_new = torch.load(os.path.join(model_dir, 'ckpt.model'))\n",
        "outputs = testing(test_loader, model, device)\n",
        "\n",
        "# 写入 csv 档案\n",
        "tmp = pd.DataFrame({\"id\": [str(i) for i in range(len(test_x))], \"labels\": outputs})\n",
        "print(\"save csv ...\")\n",
        "tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n",
        "print(\"Finishing Predicting\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading testing data ...\n",
            "Get embedding ...\n",
            "loading word to vec model ...\n",
            "get words #24694\n",
            "total words: 24696\n",
            "\n",
            "load model ...\n",
            "save csv ...\n",
            "Finishing Predicting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfOsvCHU1uBb",
        "outputId": "f8ee7f8e-968c-4aaa-e025-45841ad6a452"
      },
      "source": [
        "!pwd\n",
        "!ls"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/2020/hw4\n",
            "ckpt.model  hw4.ipynb\t testing_data.txt    training_nolabel.txt\n",
            "data.zip    predict.csv  training_label.txt  w2v_all.model\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}